from bs4 import BeautifulSoup, element
import re
import random
import math
import shutil

def valid_id(css_id):
  # All valid ids start with hix (generated by Hindenburg)
  pattern = re.compile('hix[0-9]+')
  return bool(pattern.match(str(css_id)))

def markup(foldername, bookname):
  # Open the book
  with open('././public/uploads/{}/{}.html'.format(foldername, bookname), 'r', encoding='utf8') as f:
    book = f.read()

  # Turn it into soup
  soup = BeautifulSoup(book, 'html.parser')

  # Check for rerun
  rerun = soup.find(re.compile('span'), class_='sentence', id=re.compile('[a-z]+_[0-9]+'))

  if not rerun:
    # Get all the paragraphs with valid id
    paragraphs = soup.find_all(re.compile('p|li|td|th|dt|dd'), id=valid_id)

    # To ensure that a long book with many paragraphs will always have an identifier that fits
    z_fill_len = int(math.log10(len(paragraphs)) + 1) + 1

    # Having fun with the prefix, names of co-workers
    prefix = ['marin', 'alfred', 'gunnar', 'hafthor', 'hulda', 'sigrun', 'einar']
    r_prefix = random.choice(prefix)
    n_suffix = 1

    for p_index, p in enumerate(paragraphs):
      # Check for subparagraphs (Evil thing)
      subparagraphs = BeautifulSoup(str(p.contents), 'html.parser').find_all(re.compile('p|li|td|th|dt|dd'), id=valid_id)
      sentences = ''
      if subparagraphs:
        # Since people dare to put identifying content within identifying content
        # it gets treated as a singular thing by removing newlines
        sentences = ''.join(str(t).replace('\n', '') for t in p.contents)
      else:
        sentences = ''.join([str(t) for t in p.contents])
      # Clear the paragraph of content
      p.clear()
      # Valid URL characters A-Z, a-z, 0-9, -, ., _, ~, :, /, ?, #, [, ], @, !, $, &, ', (, ), *, +, ,, ;, %, and =.
      contains_links = re.findall(r'(<a href\=\"[\w\.\:\d\/\?\&\-\=\_\~\#\[\]\@\!\$\'\(\)\*\+\,\;\%]+\">[\w\.\:\d\/\?\&\-\=\_\~\#\[\]\@\!\$\'\(\)\*\+\,\;\%]+</a>)', sentences)
      # Generate a list of sentences using regex string
      sentences = re.split(r'([^\.\?\!]+\w*\s*\.+(<\/\w+>)*\s*|[^\.\?\!]+\w*\s*\?+(<\/\w+>)*\s*|[^\.\?\!]+\w*\s*\!+(<\/\w+>)*\s*|[^\n](<.+>)*.+(<\/\w+>)*)', sentences)
      # Remove all empty elements in array
      sentences = list(filter(None, sentences))
      # Links have numerous '.' inside of them but we shouldn't split them up, so here we are putting them back together.
      for link in contains_links:
        a_opening_index = None
        a_closing_index = None
        for s_index, sentence in enumerate(sentences):
          if a_opening_index == None:
            if re.findall(r'<a href', sentence):
              a_opening_index = s_index
          if a_closing_index == None:
            if re.findall(r'</a>', sentence):
              a_closing_index = s_index
        sentences[a_opening_index:a_closing_index+1] = [''.join(sentences[a_opening_index:a_closing_index+1])]
      # For each sentence create a new span with a unique id and class="sentence" and reinsert it into p
      for s_index, sentence in enumerate(sentences):
        if sentence != '\n' and sentence != '':
          new_tag = soup.new_tag('span')
          new_tag.attrs['id'] = r_prefix + '_' + str(n_suffix).zfill(z_fill_len)
          new_tag.attrs['class'] = 'sentence'
          new_tag.insert(s_index, BeautifulSoup(str(sentence), 'html.parser'))
          p.insert(s_index, new_tag)
          n_suffix += 1

    # Write out processed book
    with open('././public/uploads/{}/{}.html'.format(foldername, bookname), 'w', encoding='utf8') as f:
      f.write(soup.decode("utf8"))
    
    # Since we are copying the file with nodejs to output but working with the upload file
    # We need to replace the copied file with our processed file
    shutil.copy("././public/uploads/{}/{}.html".format(foldername, bookname), "././public/output/{}/{}.html".format(foldername, bookname))